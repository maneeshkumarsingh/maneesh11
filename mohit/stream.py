import pandas as pdimport streamlit as stimport jsonimport loggingpd.options.mode.chained_assignment = Nonefrom sklearn.ensemble import RandomForestClassifier#from sklearn.model_selection import GridSearchCVfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfVectorizerst.title("Reade and Merge Training data ")with open('Spam_Ham_Config.json') as config_file:  config = json.load(config_file)TrainingData_path = config["TRAINING_DATA_PATH"]Testingsourcepath = config["TESTING_DATA_PATH"]# Clustingsourcepath = config['CLUSTER_DATA_PATH']resultpath = config['RESULT_PATH']Log_path = config['LOG_PATH']# Log_tag = config['LOG_TAG']# vectorizer_save = config['VECTORIZER']# model_save = config['MODEL']Spam_File_enable = config['PROCESS_FILE_EXTENSION']processed_path = config['PROCESSED_DATA_PATH']# clusteringnlog_path = config['CLUSTRINGLOG_PATH']File_Extension = config['PROCESS_FILE_EXTENSION']nltk_path = config['NLTK_PATH']logging.basicConfig(filename=Log_path, level=logging.INFO, format='%(asctime)s|%(levelname)s:%(lineno)d]%(message)s')training_df = pd.read_csv(TrainingData_path, encoding='iso-8859-1', names=['labels','training_messages'],engine='c')st.write('training data shape = ',training_df.shape)uploaded_file = st.file_uploader("Choose a file")if uploaded_file is not None:  df = pd.read_csv(uploaded_file, names=['labels','training_messages'], encoding= 'unicode_escape')  st.write('Uploded data shape = ',df.shape)  vertical_concat = pd.concat([training_df, df], axis=0)  st.write('mearge data shape = ',vertical_concat.shape)  st.write(vertical_concat)