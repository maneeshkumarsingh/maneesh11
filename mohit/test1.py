#import streamlit as stimport seaborn as snsimport pandas as pdimport refrom datetime import datetime, timedeltafrom nltk.corpus import stopwordsimport numpy as npimport jsonimport loggingpd.options.mode.chained_assignment = Nonefrom sklearn.ensemble import RandomForestClassifier#from sklearn.model_selection import GridSearchCVfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import confusion_matrix, accuracy_scorefrom sklearn.metrics import precision_recall_curve, auc, roc_curveimport matplotlib.pyplot as pltfrom sklearn.metrics import precision_recall_fscore_support, classification_report,confusion_matrix, precision_recall_curvewith open('Spam_Ham_Config.json') as config_file:  config = json.load(config_file)TrainingData_path = config["TRAINING_DATA_PATH"]#Testingsourcepath = config["TESTING_DATA_PATH"]#resultpath = config['RESULT_PATH']Log_path = config['LOG_PATH']#Spam_File_enable = config['PROCESS_FILE_EXTENSION']#processed_path = config['PROCESSED_DATA_PATH']# clusteringnlog_path = config['CLUSTRINGLOG_PATH']#File_Extension = config['PROCESS_FILE_EXTENSION']nltk_path = config['NLTK_PATH']logging.basicConfig(filename=Log_path, level=logging.INFO, format='%(asctime)s|%(levelname)s:%(lineno)d]%(message)s')def text_process(mess):    try:        assert(type(mess) == str)        cleaned = re.sub(r'\b[\w\-.]+?@\w+?\.\w{2,4}\b', 'emailaddr', mess)        cleaned = re.sub(r'(http[s]?\S+)|(\w+\.[A-Za-z]{2,4}\S*)', 'httpaddr', cleaned)        cleaned = re.sub(r'\b(\+\d{1,2}\s)?\d?[\-(.]?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b','phonenumbr', cleaned)        cleaned = re.sub(r'\d+(\.\d+)?', ' ', cleaned)        cleaned = re.sub(r'[^\w\d\s]', ' ', cleaned)        cleaned = re.sub(r'\s+', ' ', cleaned)        cleaned = re.sub(r'^\s+|\s+?$', '', cleaned.lower())        nopunc=''.join(cleaned)        return ' '.join(word.lower() for word in nopunc.split() if word not in stopwords.words('english') if len(word) != 1)    except Exception as e:        print("Exception in loading Text Process", e)        logging.error(f'Exception in loading Text Process:{e}')# This function plots the confusion matrices given y_i, y_i_hat.def plot_confusion_matrix(test_y, predict_y):    C = confusion_matrix(test_y, predict_y)    A = (((C.T) / (C.sum(axis=1))).T)    B = (C / C.sum(axis=0))    plt.figure(figsize=(20, 4))    labels = [1, 2]    # representing A in heatmap format    cmap = sns.light_palette("blue")    plt.subplot(1, 3, 1)    sns.heatmap(C, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)    plt.xlabel('Predicted Class')    plt.ylabel('Original Class')    plt.title("Confusion matrix")    plt.subplot(1, 3, 2)    sns.heatmap(B, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)    plt.xlabel('Predicted Class')    plt.ylabel('Original Class')    plt.title("Precision matrix")    plt.subplot(1, 3, 3)    # representing B in heatmap format    sns.heatmap(A, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)    plt.xlabel('Predicted Class')    plt.ylabel('Original Class')    plt.title("Recall matrix")    plt.show()def main():    training_df = pd.read_csv(TrainingData_path, encoding='iso-8859-1', names=['labels', 'training_messages'], engine='c')    #training_df['labels'] = training_df['labels'].str.upper()    training_df['labels'] = training_df['labels'].str.upper()    training_df.dropna(axis=0, inplace=True)    training_df = training_df.drop_duplicates(keep='first')    training_df.reset_index(inplace=True)    training_df['labels'] = training_df['labels'].map({'HAM': 0, 'SPAM': 1})    training_df['training_messages'] = training_df['training_messages'].apply(text_process)    # logging.critical('Trainingdata after text_process :{training_df.head(100)}.....')    # training_df[['training_messages']].to_csv("Result.csv"), analyzer=text_process    # vectorizer = TfidfVectorizer(ngram_range=(1, 2))    vectorizer = TfidfVectorizer()  # max_features=2500, min_df=7, max_df=0.8)    X_ngrams = vectorizer.fit_transform(training_df['training_messages'])    #################################################    x_train, x_test, y_train, y_test = train_test_split(X_ngrams, training_df['labels'],shuffle =True,test_size=0.2,            random_state=42, stratify=training_df['labels'])    # spam_detect_model = RandomForestClassifier(n_estimators=100).fit(X_ngrams, training_df['labels'])    model = RandomForestClassifier(n_estimators=250, random_state=0)    model.fit(x_train, y_train)    #st.write("ML Model is Trained successfully.....")    preds = model.predict(x_test)    accuracy = accuracy_score(preds, y_test)    #st.write('accuracy = ', accuracy)    #predicted_y = np.argmax(preds, axis=1)    plot_confusion_matrix(y_test,preds)    print(classification_report(y_test,preds))    #logging.critical('ML Model is Trained successfully.....')    def spam_filter(message):        if model.predict(vectorizer.transform([text_process(message)])):            return 'spam'        else:            return 'ham'    #st.write('Spam Ham DETECTION1')main()