import pandas as pdimport reimport nltkimport streamlit as stimport timeimport shutilimport loggingimport configparserimport warningsfrom string import digitsfrom logging.handlers import RotatingFileHandlerfrom datetime import datetime, timedeltafrom nltk.corpus import stopwords#from config import TrainingData, Add_TrainingData, model_save, vectorizer_save, Time_intervalfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.feature_extraction.text import TfidfVectorizerimport picklefrom scipy import sparseimport jsonwith open('Spam_Ham_Config.json') as config_file:    config = json.load(config_file)# logging_streamlit = config['STREAMLIT_LOG_PATH']Vectorizer_save = config['VECTORIZER']model_save = config['MODEL']trainingData = config['TRAINING_DATA_PATH']logpath = config['LOG_PATH']# P2P_File_enable = config['PROCESS_FILE_EXTENSION']logging.basicConfig(filename=logpath, level=logging.INFO, format='%(asctime)s|%(levelname)s:%(lineno)d]%(message)s')# Streamlit Web App#Log_path = config['Model_configration']['LOG_PATH']def text_process(mess):    try:        assert(type(mess) == str)        cleaned = re.sub(r'\b[\w\-.]+?@\w+?\.\w{2,4}\b', 'emailaddr', mess)        cleaned = re.sub(r'(http[s]?\S+)|(\w+\.[A-Za-z]{2,4}\S*)', 'httpaddr', cleaned)        cleaned = re.sub(r'\b(\+\d{1,2}\s)?\d?[\-(.]?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b','phonenumbr', cleaned)        cleaned = re.sub(r'[^\w\d\s]', ' ', cleaned)        cleaned = re.sub(r'\s+', ' ', cleaned)        cleaned = re.sub(r'^\s+|\s+?$', '', cleaned.lower())        nopunc=''.join(cleaned)        return ' '.join(word.lower() for word in nopunc.split() if word not in stopwords.words('english') if len(word) != 1)    except Exception as e:        print("Exception in loading Text Process", e)        logging.error(f'Exception in loading Text Process:{e}')def app():    #global training_df    try:        st.title('Re_Training ML Model')        st.subheader('Existing training data set')        training_df1 = pd.read_csv(trainingData, encoding='iso-8859-1', names=['labels', 'training_messages'], engine='c')        st.write(training_df1)        st.write('training data shape = ', training_df1.shape)        st.write('Spam  and Ham Count in Training Data Set')        training_df1['labels'] = training_df1['labels'].str.upper()        st.write(training_df1.labels.value_counts())        st.subheader("Uploade New Training Data Set")        uploaded_file = st.file_uploader("Choose a file")        if uploaded_file is not None:            new_df = pd.read_csv(uploaded_file, encoding='iso-8859-1', names=['labels', 'training_messages'], engine='c')  # , encoding='unicode_escape')            st.write('Uploded data shape = ', new_df.shape)            new_df['labels'] = new_df['labels'].str.upper()            st.write(new_df)            st.write('Spam and Ham Count in  Uploded Training Data Set :smiley:')            st.write(new_df.labels.value_counts())            training_df = pd.concat([training_df1, new_df], axis=0)            training_df.reset_index(inplace=True)            training_df = training_df[['labels', 'training_messages']]            st.write('mearge data shape = ', training_df.shape)            st.write(training_df)            training_df['labels'] = training_df['labels'].str.upper()            st.write('Spam and Ham Count in  Meagre Training Data Set :smiley:')            st.write(training_df.labels.value_counts())            # st.write('HAM = ', training_df.HAM.value_counts())            st.write('Successfull mearge both Training Data :smiley: ')            st.write('Start Text Cleaning and Vectorization :smiley:')            training_df['training_messages'] = training_df['training_messages'].apply(text_process)            vectorizer = TfidfVectorizer()  # max_features=2500, min_df=7, max_df=0.8)            X_ngrams = vectorizer.fit_transform(training_df['training_messages'])            st.write('Model Training Start :smiley:')            model = RandomForestClassifier(n_estimators=250, random_state=0)            model.fit(X_ngrams, training_df['labels'])            st.write("ML Model is Trained successfully :smiley:")            #logging.info('DB Connection successful')            logging.info('ML Model is Trained successfully :smiley: ')           # Save the vectorizer            pickle.dump(vectorizer, open(Vectorizer_save, 'wb'))            # Saving Model            pickle.dump(model, open(model_save, 'wb'))            st.write('Model Pickal file Updated :smiley:')            training_df.to_csv(trainingData )            st.write('Training data  updated :smiley:')    except Exception as exception:        print("Exception in training Model", exception)        logging.info(f'Exception in training model:{exception}')#if __name__ == '__main__':#    train()