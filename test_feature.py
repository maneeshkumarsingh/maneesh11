import pandas as pdimport reimport nltkimport stringimport timeimport shutilimport loggingimport configparserimport warningsfrom string import digitsfrom logging.handlers import RotatingFileHandlerfrom datetime import datetime, timedeltafrom nltk.corpus import stopwords#from config import TrainingData, Add_TrainingData, model_save, vectorizer_save, Time_intervalfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.feature_extraction.text import TfidfVectorizerimport picklefrom scipy.sparse import hstack, csr_matrixfrom scipy import sparseimport jsonwith open('Spam_Ham_Config.json') as config_file:    config = json.load(config_file)trainingData = config['TRAINING_DATA_PATH']vectorizer_save = config['VECTORIZER']model_save = config['MODEL']logpath = config['LOG_PATH']# P2P_File_enable = config['PROCESS_FILE_EXTENSION']vectorizer = pickle.load(open(vectorizer_save, 'rb'))spam_ham_detect_model = pickle.load(open(model_save, 'rb'))def meta_feature(data):    data["word_count"] = data["user_data"].apply(lambda x : len(x.split()))    #data["word_count_cleand"] = data["cleaned"].apply(lambda x : len(x.split()))    data["char_count"] = data["user_data"].apply(lambda x : len(x))    data['name_length']  = data['user_data'].str.len()    return datadef Uppercase_count(mess):    try:        count = 0        #print("insideUppercase_count")        #print(mess)        for char in str(mess):            #print(char)            if(char.isupper()):                count = count + 1                #print(count)        return count    except Exception as e:        print("Exception in Uppercase_count",e)        #logging.error(f'Exception in Uppercase_count:{e}')def process_Predict(chunk):    global vectorizer    #global vocab    global spam_ham_detect_model    # global count    try:        #chunk['user_data'] = chunk['user_data']        chunk['CapCount'] = chunk['user_data'].apply(Uppercase_count)        # count = 0        #chunk['CapCount'] = chunk['OrigUserData'].apply(Uppercase_count)        ##############################################################################        chunk = meta_feature(chunk)        X_ngrams = vectorizer.transform(chunk['user_data'].values.astype('U'))        meta_features = ['word_count', 'char_count', 'name_length', 'CapCount']        feature_set1 = chunk[meta_features]        messages_tfidf_predict = hstack([X_ngrams, csr_matrix(feature_set1)], "csr")       #######################################################################################        #messages_tfidf_predict = tfidf_transformer.transform(messages_bow_predict)        prediction = spam_ham_detect_model.predict(messages_tfidf_predict)        ham_prob = spam_ham_detect_model.predict_proba(messages_tfidf_predict)[:, 0]        spam_prob = spam_ham_detect_model.predict_proba(messages_tfidf_predict)[:, 1]        chunk['spam/ham'] = prediction        chunk['spam_prob'] = spam_prob        chunk['ham_prob'] = ham_prob        chunk['spam/ham'] = chunk['spam/ham'].map({0:'HAM', 1:'SPAM'})        chunk = chunk.drop(['word_count', 'char_count', 'name_length', 'CapCount'], axis=1)        return chunk    except Exception as exception:        print("Exception in process_Predict:", exception)        logging.error(f'Exception in process_Predict:{exception}')def text_process(mess):    try:        assert(type(mess) == str)        cleaned = re.sub(r'\b[\w\-.]+?@\w+?\.\w{2,4}\b', 'emailaddr', mess)        cleaned = re.sub(r'(http[s]?\S+)|(\w+\.[A-Za-z]{2,4}\S*)', 'httpaddr', cleaned)        cleaned = re.sub(r'\b(\+\d{1,2}\s)?\d?[\-(.]?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b','phonenumbr', cleaned)        cleaned = re.sub(r'[^\w\d\s]', ' ', cleaned)        cleaned = re.sub(r'\s+', ' ', cleaned)        cleaned = re.sub(r'^\s+|\s+?$', '', cleaned.lower())        nopunc=''.join(cleaned)        return ' '.join(word.lower() for word in nopunc.split() if word not in stopwords.words('english') if len(word) != 1)    except Exception as e:        print("Exception in loading Text Process", e)        logging.error(f'Exception in loading Text Process:{e}')def train():    #global training_df    try:        training_df = pd.read_csv(trainingData, encoding='iso-8859-1', names=['labels', 'user_data'], engine='c')        #AddingData_df = pd.read_csv(add_trainingData, names=['labels', 'training_messages'])        #training_df = trainingdf.append(AddingData_df, ignore_index=True)        training_df.dropna(axis=0, inplace=True)        training_df = training_df.drop_duplicates(keep='first')        training_df.reset_index(inplace=True)        logging.info('Loading Training Data successfull....')        print(('Loading Training Data successfull....'))        training_df['labels'] = training_df['labels'].str.upper()        training_df['labels'] = training_df['labels'].map({'HAM': 0, 'SPAM': 1})        training_df['CapCount'] = training_df['user_data'].apply(Uppercase_count)        training_df['user_data'] = training_df['user_data'].apply(text_process)        training_df = meta_feature(training_df)        vectorizer = TfidfVectorizer(max_features=2500, min_df=7, max_df=0.8)        X_ngrams = vectorizer.fit_transform(training_df['user_data'])        #################################################        meta_features = ['word_count', 'char_count', 'name_length', 'CapCount']        feature_set1 = training_df[meta_features]        train = hstack([X_ngrams, csr_matrix(feature_set1)], "csr")        spam_ham_detect_model = RandomForestClassifier(n_estimators=100).fit(train, training_df['labels'])        print("ML Model is Trained successfully.....")        #logging.info('DB Connection successful')        logging.info('ML Model is Trained successfully.....')        message = "GHS 0.01 debited from Xpress A/C - Bank2Wallet 233243931614 on 22-Jul-19. Your Available Balance is GHS 0.00"        data ={'user_data':[message]}        df = pd.DataFrame(data)        pred1 = process_Predict(df)        pred1 = pred1.head(1)        pred = pred1['spam/ham'][0]        print(pred)    except Exception as exception:        print("Exception in training model", exception)        logging.info(f'Exception in training model:{exception}')if __name__ == '__main__':    train()